@inproceedings{coates2011analysis,
  title={An analysis of single-layer networks in unsupervised feature learning},
  author={Coates, Adam and Ng, Andrew Y and Lee, Honglak},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={215--223},
  year={2011}
}

@inproceedings{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y}
}
@inproceedings{lin2014stable,
  title={Stable and efficient representation learning with nonnegativity constraints},
  author={Lin, Tsung-Han and Kung, HT},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  pages={1323--1331},
  year={2014}
}
@article{lee2014deeply,
  title={Deeply-supervised nets},
  author={Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
  journal={arXiv preprint arXiv:1409.5185},
  year={2014}
}
@inproceedings{mairal2014convolutional,
  title={Convolutional kernel networks},
  author={Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2627--2635},
  year={2014}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@InProceedings{WangSLRWPCW2014,
author={Jiang Wang and Yang Song and Thomas Leung and Chuck Rosenberg and Jingbin Wang and James Philbin and Bo Chen and Ying Wu},
title={Learning fine-grained image similarity with deep ranking},
booktitle={CVPR},
year=2014}

@InProceedings{shamir,
  author =    {Omer Tamuz and Ce Liu and Serge Belongie and Ohad Shamir and Adam Kalai},
  title =     {Adaptively Learning the Crowd Kernel },
  booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  series =    {ICML '11},
  year =      {2011},
  editor =    {Lise Getoor and Tobias Scheffer},
  location =  {Bellevue, Washington, USA},
  isbn =      {978-1-4503-0619-5},
  month =     {June},
  publisher = {ACM},
  address =   {New York, NY, USA},
  pages=      {673--680},
}
@article{zeiler2013stochastic,
  title={Stochastic pooling for regularization of deep convolutional neural networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  journal={arXiv preprint arXiv:1301.3557},
  year={2013}
}
@article{goodfellow2013maxout,
  title={Maxout networks},
  author={Goodfellow, Ian J and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1302.4389},
  year={2013}
}



@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  journal={Computer Science Department, University of Toronto, Tech. Rep},
  year={2009},
  publisher={Citeseer}
}
@article{chechik2010large,
  title={Large scale online learning of image similarity through ranking},
  author={Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={1109--1135},
  year={2010},
  publisher={JMLR. org}
}
@article{inception,
  author    = {Christian Szegedy and
               Wei Liu and
               Yangqing Jia and
               Pierre Sermanet and
               Scott Reed and
               Dragomir Anguelov and
               Dumitru Erhan and
               Vincent Vanhoucke and
               Andrew Rabinovich},
  title     = {Going Deeper with Convolutions},
  journal   = {CoRR},
  volume    = {abs/1409.4842},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.4842},
  timestamp = {Wed, 01 Oct 2014 15:00:05 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SzegedyLJSRAEVR14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{mobahi2009deep,
  title={Deep learning from temporal coherence in video},
  author={Mobahi, Hossein and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={737--744},
  year={2009},
  organization={ACM}
}

@article{LinCY13,
  author    = {Min Lin and
               Qiang Chen and
               Shuicheng Yan},
  title     = {Network In Network},
  journal   = {CoRR},
  volume    = {abs/1312.4400},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.4400},
  timestamp = {Mon, 06 Jan 2014 15:10:41 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LinCY13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Goodfellow2013,
abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout’s fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR- 100, and SVHN},
archivePrefix = {arXiv},
arxivId = {arXiv:1302.4389v4},
author = {Goodfellow, Ian J and Warde-farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
eprint = {arXiv:1302.4389v4},
pages = {1319--1327},
title = {{Maxout Networks}},
year = {2013}
}

@inproceedings{Bengio2013,
abstract = {Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges.},
archivePrefix = {arXiv},
arxivId = {1305.0445},
author = {Bengio, Yoshua},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-39593-2\_1},
eprint = {1305.0445},
isbn = {9783642395925},
issn = {03029743},
pages = {1--37},
title = {{Deep learning of representations: Looking forward}},
volume = {7978 LNAI},
year = {2013}
}


@misc{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
booktitle = {Foundations and Trends® in Machine Learning},
doi = {10.1561/2200000006},
eprint = {0500581},
isbn = {2200000006},
issn = {1935-8237},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}


@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
eprint = {1102.0183},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}

@article{Zeiler2013,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {1311.2901},
journal = {arXiv preprint arXiv:1311.2901},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://arxiv.org/abs/1311.2901},
year = {2013}
}








@misc{Hinton2007,
abstract = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Hinton, Geoffrey E.},
booktitle = {Trends in Cognitive Sciences},
doi = {10.1016/j.tics.2007.09.004},
isbn = {1364-6613 (Print)$\backslash$r1364-6613 (Linking)},
issn = {13646613},
pages = {428--434},
pmid = {17921042},
title = {{Learning multiple layers of representation}},
volume = {11},
year = {2007}
}




@article{Bengio2013,
abstract = {Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges.},
archivePrefix = {arXiv},
arxivId = {1305.0445},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-39593-2\_1},
eprint = {1305.0445},
isbn = {978-3-642-39592-5},
issn = {18684394},
journal = {arXiv preprint arXiv:1305.0445},
pages = {1--37},
title = {{Deep Learning of Representations: Looking Forward}},
url = {http://arxiv.org/abs/1305.0445$\backslash$nhttp://dx.doi.org/10.1007/978-3-642-39593-2\_1},
volume = {7978},
year = {2013}
}







@article{Zeiler20132,
abstract = {We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.},
archivePrefix = {arXiv},
arxivId = {1301.3557},
author = {Zeiler, Matthew and Fergus, Rob},
eprint = {1301.3557},
journal = {arXiv preprint arXiv:1301.3557},
pages = {1--9},
title = {{Stochastic pooling for regularization of deep convolutional neural networks}},
url = {http://arxiv.org/abs/1301.3557},
year = {2013}
}






@article{Lin2013,
abstract = {We propose a novel network structure called "Network In Network" (NIN) to enhance the model discriminability for local receptive fields. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to handle the variance of the local receptive fields. We instantiate the micro neural network with a nonlinear multiple layer structure which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner of CNN and then fed into the next layer. The deep NIN is thus implemented as stacking of multiple sliding micro neural networks. With the enhanced local modeling via micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is more interpretable and less prone to overfitting than traditional fully connected layers. We demonstrated state-of-the-art classification performances with NIN on CIFAR-10, CIFAR-100 and SVHN datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
journal = {arXiv:1312.4400 [cs]},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400$\backslash$nhttp://www.arxiv.org/pdf/1312.4400.pdf},
year = {2013}
}


@article{Sermanet,
abstract = {We present an integrated framework for using ConvolutionalNetworks for classi- fication, localization and detection.We showhowamultiscale and slidingwindow approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object bound- aries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simul- taneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNetLarge ScaleVisual RecognitionChallenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competitionwork, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
journal = {arXiv preprint arXiv:1312.6229},
pages = {1--15},
title = {{OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}


@article{Szegedy,
  author    = {Christian Szegedy and
               Wojciech Zaremba and
               Ilya Sutskever and
               Joan Bruna and
               Dumitru Erhan and
               Ian J. Goodfellow and
               Rob Fergus},
  title     = {Intriguing properties of neural networks},
  journal   = {CoRR},
  volume    = {abs/1312.6199},
  year      = {2013},
  ee        = {http://arxiv.org/abs/1312.6199},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@incollection{Weston2012,
abstract = {We show how nonlinear embedding algorithms popular for use with “shallow” semi-supervised learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This trick provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.},
author = {Weston, Jason and Ratle, Frederic and Mobahi, Hossein and Collobert, Ronan},
booktitle = {Neural Networks: Tricks of the Trade},
doi = {10.1145/1390156.1390303},
isbn = {9781605582054},
keywords = {deep learning,embedding,semi-supervised learning},
pages = {639--655},
title = {{Deep Learning via Semi-Supervised Embedding}},
year = {2012}
}





@incollection{LeCun1998,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Y. and Bottou, L. and Orr, G. and Muller, K.},
booktitle = {Neural Networks: Tricks of the trade},
doi = {10.1007/3-540-49430-8\_2},
isbn = {978-3-540-65311-0},
issn = {08936080},
pages = {9--50},
title = {{Efficient BackProp}},
url = {http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf},
year = {1998}
}

@misc{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
booktitle = {Nature},
doi = {10.1038/323533a0},
isbn = {0262661160},
issn = {0028-0836},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{LeCun2010,
abstract = {Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or "features")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.},
author = {LeCun, Yann LeCun Yann and Kavukcuoglu, K. and Farabet, C.},
doi = {10.1109/ISCAS.2010.5537907},
isbn = {978-1-4244-5308-5},
issn = {02714302},
journal = {Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on},
title = {{Convolutional networks and applications in vision}},
year = {2010}
}

@article{Sermanet2012,
abstract = {We classify digits of real-world house numbers using convolutional neural networks (ConvNets). ConvNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditionalConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10\% accuracy on the SVHN dataset (48\% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.},
archivePrefix = {arXiv},
arxivId = {1204.3968},
author = {Sermanet, Pierre and Chintala, Soumith and LeCun, Yann},
eprint = {1204.3968},
isbn = {978-1-4673-2216-4},
issn = {1051-4651},
journal = {Proceedings of International Conference on Pattern Recognition ICPR12},
pages = {10--13},
title = {{Convolutional neural networks applied to house numbers digit classification}},
url = {http://arxiv.org/abs/1204.3968$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6460867},
year = {2012}
}

@article{Collobert2014,
abstract = {Scene parsing is a technique that consist on giving a label to all pixels in an image according to the class they belong to. To ensure a good visual coherence and a high class accuracy, it is essential for a scene parser to capture image long range dependencies. In a feed-forward architecture, this can be simply achieved by considering a sufficiently large input context patch, around each pixel to be labeled. We propose an approach consisting of a recurrent convolutional neural network which allows us to consider a large input context, while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation methods, nor any task-specific features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identifies and corrects its own errors. Our approach yields state-of-the-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time.},
archivePrefix = {arXiv},
arxivId = {1306.2795},
author = {Collobert, Ronan and Pinheiro, PHO},
eprint = {1306.2795},
journal = {Proceedings of The 31st International Conference on Machine Learning},
pages = {82--90},
title = {{Recurrent Convolutional Neural Networks for Scene Labeling}},
url = {http://infoscience.epfl.ch/record/192577/files/Pinheiro\_Idiap-RR-41-2013.pdf},
year = {2014}
}


@article{Farabet2012,
abstract = {Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image. The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average "purity" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free. The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 $\backslash$times 240 image labeling in less than 1 second.},
archivePrefix = {arXiv},
arxivId = {1202.2160},
author = {Farabet, Cl\'{e}ment and Couprie, Camille and Najman, Laurent and LeCun, Yann},
eprint = {1202.2160},
isbn = {978-1-4503-1285-1},
issn = {<null>},
title = {{Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers}},
url = {http://arxiv.org/abs/1202.2160},
year = {2012}
}

@article{Razavian2014,
abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Remarkably we report better or competitive results compared to the state-of-the-art in all the tasks on various datasets. The results are achieved using a linear SVM classifier applied to a feature representation of size 4096 extracted from a layer in the net. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual classification tasks.},
archivePrefix = {arXiv},
arxivId = {1403.6382},
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
eprint = {1403.6382},
journal = {Arxiv},
title = {{CNN Features off-the-shelf: an Astounding Baseline for Recognition}},
url = {http://arxiv.org/abs/1403.6382},
year = {2014}
}
@article{Arel2010,
abstract = {This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and "weaknesses, depending on the application and context in "which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.},
archivePrefix = {arXiv},
arxivId = {1209.5467},
author = {Arel, Itamar and Rose, Derek C and Karnowski, Thomas P},
doi = {10.1109/MCI.2010.938364},
eprint = {1209.5467},
isbn = {1556-603X},
issn = {1556-603X},
journal = {IEEE Computational Intelligence Magazine},
pages = {13--18},
title = {{Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier]}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5605630},
volume = {5},
year = {2010}
}

@article{Baldi2013,
abstract = {Dropout is a relatively new algorithm for training neural networks which relies on stochastically “dropping out” neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for study- ing dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both lin- ear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approx- imation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. We also show in simple cases how dropout performs stochastic gra- dient descent on a regularized error function. 1},
author = {Baldi, Pierre and Sadowski, Peter},
journal = {NIPS},
pages = {1--9},
title = {{Understanding Dropout}},
url = {http://www.editlib.org/INDEX.CFM?fuseaction=Reader.ViewAbstract\&paper\_id=4620},
year = {2013}
}


@article{Salakhutdinov2009,
abstract = {We present a new learning algorithm for Boltz- mann machines that contain many layers of hid- den variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data- independent expectations are approximated us- ing persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden lay- ers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pre-training phase that allows variational in- ference to be initialized with a single bottom- up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and per- form well on handwritten digit and visual object recognition tasks},
archivePrefix = {arXiv},
arxivId = {1203.4416},
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
eprint = {1203.4416},
file = {::},
journal = {Artificial Intelligence},
pages = {448--455},
title = {{Deep Boltzmann Machines}},
url = {http://www.cs.utoronto.ca/~rsalakhu/papers/dbm.pdf},
volume = {5},
year = {2009}
}


@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G E and Salakhutdinov, R R},
doi = {10.1126/science.1127647},
isbn = {3135786504},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the dimensionality of data with neural networks.}},
volume = {313},
year = {2006}
}


@article{Vincent2008,
abstract = {Previous work has shown that the difficul- ties in learning deep generative or discrim- inative models can be overcome by an ini- tial unsupervised learning step that maps in- puts to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a rep- resentation based on the idea of making the learned representations robust to partial cor- ruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to ini- tialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising ad- vantage of corrupting the input of autoen- coders on a pattern classification benchmark suite.},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1145/1390156.1390294},
isbn = {9781605582054},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
pages = {1096--1103},
title = {{Extracting and composing robust features with denoising autoencoders}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390294},
year = {2008}
}

@article{Szlam2012,
abstract = {We describe a method for fast approximation of sparse coding. The input space is subdivided by a binary decision tree, and we simultaneously learn a dictionary and assignment of allowed dictionary elements for each leaf of the tree. We store a lookup table with the assignments and the pseudoinverses for each node, allowing for very fast inference. We give an algorithm for learning the tree, the dictionary and the dictionary element assignment, and In the process of describing this algorithm, we discuss the more general problem of learning the groups in group structured sparse modelling. We show that our method creates good sparse representations by using it in the object recognition framework of $\backslash$cite\{lazebnik06,yang-cvpr-09\}. Implementing our own fast version of the SIFT descriptor the whole system runs at 20 frames per second on \$321 \backslash times 481\$ sized images on a laptop with a quad-core cpu, while sacrificing very little accuracy on the Caltech 101 and 15 scenes benchmarks.},
archivePrefix = {arXiv},
arxivId = {1202.6384},
author = {Szlam, Arthur and Gregor, Karol and Lecun, Yann},
doi = {10.1007/978-3-642-33715-4\_15},
eprint = {1202.6384},
isbn = {978-3-642-33714-7},
journal = {Computer Vision–ECCV 2012},
pages = {200--213},
title = {{Fast approximations to structured sparse coding and applications to object classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33715-4\_15},
year = {2012}
}
@article{Hinton20062,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
doi = {10.1162/neco.2006.18.7.1527},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
pages = {1527--1554},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets.}},
volume = {18},
year = {2006}
}


@book{Elman1996,
  added-at = {2008-09-16T23:39:07.000+0200},
  address = {Cambridge, MA},
  author = {Elman, Jeff and Bates, Elizabeth and Karmiloff-Smith, Annette and Johnson, Mark and Parisi, Domenico and Plunkett, Kim},
  biburl = {http://www.bibsonomy.org/bibtex/2ecfc6c9957465a7c8f3b8d59abf268d6/brian.mingus},
  booktitle = {Rethinking Innateness: A Connectionist Perspective on Development},
  description = {CCNLab BibTeX},
  interhash = {503b071f3ac9c545656c4b93f0830a5d},
  intrahash = {ecfc6c9957465a7c8f3b8d59abf268d6},
  keywords = {devo, object permanence},
  publisher = {MIT Press},
  timestamp = {2008-09-16T23:39:07.000+0200},
  title = {Rethinking Innateness: A Connectionist Perspective on Development},
  year = {1996}
  }



@inproceedings{Socher2011,
abstract = {Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1\%). The features from the image parse tree outperform Gist descriptors for scene classification by 4\%.},
author = {Socher,  and Lin,  and Ng,  and Manning, },
booktitle = {Neural Networks},
isbn = {9781450306195},
pages = {129--136},
title = {{Parsing Natural Scenes and Natural Language with Recursive Neural Networks}},
year = {2011}
}
@inproceedings{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
booktitle = {arXiv preprint arXiv:1301.3781},
eprint = {1301.3781},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781v3},
year = {2013}
}

@book{RumelhartBOOK1986,
abstract = {What makes people smarter than computers? The work described in these two volumes suggests that the answer lies in the massively parallel architecture of the human mind. It is some of the most exciting work in cognitive science, unifying neural and cognitive processes in a highly computational framework, with links to artificial intelligence. Although thought and problem solving have a sequential character when viewed over a time frame of minutes or hours, the authors argue that each step in the sequence is the result of the simultaneous activity of a large number of simple computational elements, each influencing others and being influenced by them. Parallel Distributed Processing describes their work in developing a theoretical framework for describing this parallel distributed processing activity and in applying the framework to the development of models of aspects of perception, memory, language, and thought. Volume 1 lays the theoretical foundations of parallel distributed processing. It introduces the approach and the reasons why the authors feel it is a fruitful one, describes several models of basic mechanisms with wide applicability to different problems, and presents a number of specific technical analyses of different aspects of parallel distributed models.},
author = {Rumelhart, David E and McClelland, James L and Williams, R J},
booktitle = {Computational models of cognition and perception},
file = {::},
isbn = {0262181207},
pages = {318--362},
pmid = {4278784},
title = {{Parallel Distributed Processing: Explorations in the Microstructure of Cognition}},
url = {http://www.amazon.com/Parallel-Distributed-Processing-Explorations-Microstructure/dp/0262181207},
volume = {1},
year = {1986}
}

@article{Cun1990,
abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
author = {Cun, Yann Le and Denker, John S and Solla, Sara A},
doi = {10.1.1.32.7223},
isbn = {1558601007},
journal = {Advances in Neural Information Processing Systems},
pages = {598--605},
title = {{Optimal Brain Damage}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/cbdv.200490137/abstract$\backslash$nhttp://www.lecun.com/exdb/publis/pdf/lecun-90b.pdf$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.7223\&amp;rep=rep1\&amp;type=pdf},
volume = {2},
year = {1990}
}

@inproceedings{Chopra2005,
abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L<sub>1</sub> norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
author = {Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2005.202},
isbn = {0769523722},
issn = {10636919},
pages = {539--546},
title = {{Learning a similarity metric discriminatively, with application to face verification}},
volume = {1},
year = {2005}
}

@inproceedings{hadsell2006dimensionality,
  title={Dimensionality reduction by learning an invariant mapping},
  author={Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  booktitle={Computer vision and pattern recognition, 2006 IEEE computer society conference on},
  volume={2},
  pages={1735--1742},
  year={2006},
  organization={IEEE}
}
@article{bromley1993signature,
  title={Signature verification using a “Siamese” time delay neural network},
  author={Bromley, Jane and Bentz, James W and Bottou, L{\'e}on and Guyon, Isabelle and LeCun, Yann and Moore, Cliff and S{\"a}ckinger, Eduard and Shah, Roopak},
  journal={International Journal of Pattern Recognition and Artificial Intelligence},
  volume={7},
  number={04},
  pages={669--688},
  year={1993},
  publisher={World Scientific}
}

@article{krizhevsky2009
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  journal={Computer Science Department, University of Toronto, Tech. Rep},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{collobert2011torch7,
  title={Torch7: A matlab-like environment for machine learning},
  author={Collobert, Ronan and Kavukcuoglu, Koray and Farabet, Cl{\'e}ment},
  booktitle={BigLearn, NIPS Workshop},
  number={EPFL-CONF-192376},
  year={2011}
}

